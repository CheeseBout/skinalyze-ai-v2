"""
RAG Cosmetic Chatbot Core - Stateless for NestJS Backend Integration
Merged Features: 
- Stateless design (NestJS handles session/history)
- Advanced Skin Condition Detection (Length-prioritized matching)
- Currency Conversion (USD -> VND)
- Smart Product Grouping & Filtering (Improved)
- VLM Skin Analysis (Base64/Bytes support)
- Improved chunk_size and format_docs logic
- Enhanced error handling from standalone
- Comprehensive prompt template
"""

import os
import re
from pathlib import Path
import torch
from PIL import Image
import google.generativeai as genai
import base64
import io
import time
from dotenv import load_dotenv
from langchain_community.document_loaders import TextLoader
from langchain_chroma import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# CONFIGURATION
PATH = Path(__file__).parent.resolve()
CHUNKS_FILE = PATH / "data" / "product_chunks.txt"
PERSIST_DIRECTORY = PATH / "db_chroma"
MODEL_NAME = "sentence-transformers/all-MiniLM-L6-v2"

# T·ª∑ gi√° USD ‚Üí VND (c·ªë ƒë·ªãnh)
USD_TO_VND = 26349

# Global cache for embeddings
_CACHED_EMBEDDINGS = None

# =============================================================================
# DATA MAPPING - EXTENDED (∆Øu ti√™n t·ª´ kh√≥a D√ÄI tr∆∞·ªõc, NG·∫ÆN sau)
# =============================================================================
SKIN_CONDITION_TO_SKIN_TYPE = {
    # ‚ö†Ô∏è ∆ØU TI√äN: T·ª´ kh√≥a D√ÄI/C·ª§ TH·ªÇ tr∆∞·ªõc, NG·∫ÆN/CHUNG sau ƒë·ªÉ tr√°nh nh·∫ßm l·∫´n
    
    # M·ª•n c√≥c (warts) - ∆ØU TI√äN TR∆Ø·ªöC "m·ª•n"
    "warts": ["H·ªón h·ª£p", "Kh√¥", "Th∆∞·ªùng", "D·∫ßu", "Nh·∫°y c·∫£m"],
    "m·ª•n c√≥c": ["H·ªón h·ª£p", "Kh√¥", "Th∆∞·ªùng", "D·∫ßu", "Nh·∫°y c·∫£m"],
    "c√≥c": ["H·ªón h·ª£p", "Kh√¥", "Th∆∞·ªùng", "D·∫ßu", "Nh·∫°y c·∫£m"],
    
    # M·ª•n tr·ª©ng c√° - SAU "m·ª•n c√≥c", TR∆Ø·ªöC "m·ª•n"
    "m·ª•n tr·ª©ng c√°": ["H·ªón h·ª£p", "D·∫ßu", "Nh·∫°y c·∫£m"],
    
    # M·ª•n (acne) - CU·ªêI C√ôNG
    "acne": ["H·ªón h·ª£p", "D·∫ßu", "Nh·∫°y c·∫£m"],
    "m·ª•n": ["H·ªón h·ª£p", "D·∫ßu", "Nh·∫°y c·∫£m"],
    
    # C√°c b·ªánh kh√°c
    "actinic keratosis": ["Kh√¥", "Th∆∞·ªùng"],
    "da d√†y s·ª´ng": ["Kh√¥", "Th∆∞·ªùng"],
    "d√†y s·ª´ng": ["Kh√¥", "Th∆∞·ªùng"],
    
    "drug eruption": ["H·ªón h·ª£p", "Kh√¥", "Th∆∞·ªùng", "D·∫ßu", "Nh·∫°y c·∫£m"],
    "ph√°t ban do thu·ªëc": ["H·ªón h·ª£p", "Kh√¥", "Th∆∞·ªùng", "D·∫ßu", "Nh·∫°y c·∫£m"],
    "ph√°t ban thu·ªëc": ["H·ªón h·ª£p", "Kh√¥", "Th∆∞·ªùng", "D·∫ßu", "Nh·∫°y c·∫£m"],
    
    "eczema": ["H·ªón h·ª£p", "Kh√¥", "Th∆∞·ªùng", "D·∫ßu", "Nh·∫°y c·∫£m"],
    "ch√†m": ["H·ªón h·ª£p", "Kh√¥", "Th∆∞·ªùng", "D·∫ßu", "Nh·∫°y c·∫£m"],
    "vi√™m da": ["H·ªón h·ª£p", "Kh√¥", "Th∆∞·ªùng", "D·∫ßu", "Nh·∫°y c·∫£m"],
    
    "psoriasis": ["Kh√¥"],
    "v·∫£y n·∫øn": ["Kh√¥"],
    
    "rosacea": ["H·ªón h·ª£p", "D·∫ßu", "Nh·∫°y c·∫£m"],
    "tr·ª©ng c√° ƒë·ªè": ["H·ªón h·ª£p", "D·∫ßu", "Nh·∫°y c·∫£m"],
    "da ƒë·ªè": ["H·ªón h·ª£p", "D·∫ßu", "Nh·∫°y c·∫£m"],
    
    "seborrheic keratoses": ["Th∆∞·ªùng", "D·∫ßu", "Nh·∫°y c·∫£m"],
    "vi√™m da ti·∫øt b√£": ["Th∆∞·ªùng", "D·∫ßu", "Nh·∫°y c·∫£m"],
    
    "sun damage": ["H·ªón h·ª£p", "Kh√¥", "Th∆∞·ªùng", "Nh·∫°y c·∫£m"],
    "h∆∞ t·ªïn do n·∫Øng": ["H·ªón h·ª£p", "Kh√¥", "Th∆∞·ªùng", "Nh·∫°y c·∫£m"],
    "t·ªïn th∆∞∆°ng n·∫Øng": ["H·ªón h·ª£p", "Kh√¥", "Th∆∞·ªùng", "Nh·∫°y c·∫£m"],
    
    "tinea": ["H·ªón h·ª£p", "D·∫ßu"],
    "n·∫•m da": ["H·ªón h·ª£p", "D·∫ßu"],
    "n·∫•m": ["H·ªón h·ª£p", "D·∫ßu"],
}

# Danh s√°ch b·ªánh da ƒë∆∞·ª£c h·ªó tr·ª£ t∆∞ v·∫•n (ƒë·ªÉ ki·ªÉm tra ph·∫°m vi)
SUPPORTED_SKIN_CONDITIONS = [
    "m·ª•n", "acne", "m·ª•n tr·ª©ng c√°",
    "ch√†m", "eczema", "vi√™m da",
    "v·∫£y n·∫øn", "psoriasis",
    "tr·ª©ng c√° ƒë·ªè", "rosacea", "da ƒë·ªè",
    "d√†y s·ª´ng", "actinic keratosis", "da d√†y s·ª´ng",
    "n·∫•m da", "tinea", "n·∫•m",
    "vi√™m da ti·∫øt b√£", "seborrheic keratoses",
    "t·ªïn th∆∞∆°ng n·∫Øng", "sun damage", "h∆∞ t·ªïn do n·∫Øng",
    "m·ª•n c√≥c", "warts", "c√≥c",
    "ph√°t ban thu·ªëc", "drug eruption", "ph√°t ban do thu·ªëc"
]

# =============================================================================
# HELPER FUNCTIONS
# =============================================================================
def setup_api_key():
    """Setup Google API Key"""
    api_key = os.getenv("GOOGLE_API_KEY")

    if not api_key:
        print("\n‚ùå CRITICAL ERROR: GOOGLE_API_KEY not found in environment variables.")
        print("Please create a .env file and add GOOGLE_API_KEY=your_new_key")
        raise ValueError("GOOGLE_API_KEY is missing.")
    
    genai.configure(api_key=api_key)
    print("‚úÖ API Key configured successfully from environment!\n")

def extract_product_name(chunk_text):
    """Tr√≠ch xu·∫•t t√™n s·∫£n ph·∫©m t·ª´ chunk text"""
    # T√¨m "Product Name: ..."
    match = re.search(r'Product Name:\s*(.+?)(?:\n|$)', chunk_text, re.IGNORECASE)
    if match:
        return match.group(1).strip()
    
    # T√¨m "T√™n s·∫£n ph·∫©m: ..."
    match = re.search(r'T√™n s·∫£n ph·∫©m:\s*(.+?)(?:\n|$)', chunk_text, re.IGNORECASE)
    if match:
        return match.group(1).strip()
    
    # Fallback: l·∫•y d√≤ng ƒë·∫ßu ti√™n c√≥ d·∫•u :
    lines = chunk_text.split('\n')
    for line in lines:
        if ':' in line:
            potential_name = line.split(':', 1)[1].strip()
            if len(potential_name) > 5:
                return potential_name
    return "Unknown Product"

def extract_field_from_chunk(chunk_text, field_name):
    """Tr√≠ch xu·∫•t gi√° tr·ªã c·ªßa field t·ª´ chunk text"""
    pattern = rf'{field_name}:\s*(.+?)(?:\n|$)'
    match = re.search(pattern, chunk_text, re.IGNORECASE)
    
    if match:
        value = match.group(1).strip()
        value = value.replace('---', '').strip()
        if value and value != 'N/A':
            return value
    return None

def convert_price_in_text(text):
    """T√¨m v√† chuy·ªÉn ƒë·ªïi gi√° USD sang VND trong text"""
    def replace_price(match):
        price_str = match.group(1)
        try:
            price_usd = float(price_str)
            price_vnd = int(price_usd * USD_TO_VND)
            return f"${price_usd:.0f} (‚âà {price_vnd:,} VND)".replace(',', '.')
        except:
            return match.group(0)
    
    result = re.sub(r'\$([0-9]+(?:\.[0-9]+)?)', replace_price, text)
    return result

def detect_skin_condition_and_types(query):
    """
    Ph√°t hi·ªán b·ªánh da trong c√¢u h·ªèi v√† tr·∫£ v·ªÅ lo·∫°i da ph√π h·ª£p
    Returns: (detected_condition, skin_types_list) ho·∫∑c (None, None)
    
    ‚ö†Ô∏è ∆ØU TI√äN: Ki·ªÉm tra t·ª´ kh√≥a D√ÄI tr∆∞·ªõc (m·ª•n c√≥c) r·ªìi m·ªõi ƒë·∫øn NG·∫ÆN (m·ª•n)
    ƒë·ªÉ tr√°nh nh·∫ßm l·∫´n khi "m·ª•n c√≥c" ch·ª©a t·ª´ "m·ª•n"
    """
    query_lower = query.lower()
    
    # S·∫Øp x·∫øp theo ƒë·ªô d√†i t·ª´ kh√≥a (d√†i -> ng·∫Øn) ƒë·ªÉ ∆∞u ti√™n match c·ª• th·ªÉ tr∆∞·ªõc
    sorted_conditions = sorted(
        SKIN_CONDITION_TO_SKIN_TYPE.items(),
        key=lambda x: len(x[0]),
        reverse=True
    )
    
    for condition, skin_types in sorted_conditions:
        if condition in query_lower:
            return condition, skin_types
    
    return None, None

def is_supported_condition(condition):
    """Ki·ªÉm tra b·ªánh da c√≥ trong danh s√°ch h·ªó tr·ª£ kh√¥ng"""
    if not condition:
        return False
    condition_lower = condition.lower()
    return any(supported in condition_lower or condition_lower in supported 
               for supported in SUPPORTED_SKIN_CONDITIONS)

# =============================================================================
# VECTOR STORE - Enhanced Error Handling
# =============================================================================
def load_or_create_vectorstore():
    """Load or create vector store with comprehensive error handling"""
    global _CACHED_EMBEDDINGS
    
    print("=" * 80)
    print("üìö KH·ªûI T·∫†O VECTOR STORE")
    print("=" * 80)
    
    db = None
    embeddings = None
    
    try:
        # ----- T·∫£i Embedding Model (v·ªõi cache) -----
        if _CACHED_EMBEDDINGS is not None:
            print(f"\n‚ö° S·ª≠ d·ª•ng cached embedding model")
            embeddings = _CACHED_EMBEDDINGS
        else:
            print(f"\n‚è≥ ƒêang t·∫£i embedding model: {MODEL_NAME}...")
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
            print(f"   üñ•Ô∏è S·ª≠ d·ª•ng thi·∫øt b·ªã: {device}")
            
            try:
                embeddings = HuggingFaceEmbeddings(
                    model_name=MODEL_NAME,
                    model_kwargs={'device': device},
                    encode_kwargs={'normalize_embeddings': True}
                )
                _CACHED_EMBEDDINGS = embeddings
                print("‚úÖ ƒê√£ t·∫£i embedding model!\n")
            except Exception as e_embed:
                print(f"\n‚ùå L·ªñI NGHI√äM TR·ªåNG khi t·∫£i embedding model: {e_embed}")
                print("   Ki·ªÉm tra l·∫°i t√™n model, k·∫øt n·ªëi m·∫°ng v√† c√†i ƒë·∫∑t th∆∞ vi·ªán.")
                return None, None

        # ----- Load ho·∫∑c T·∫°o Database -----
        should_create_new = True
        
        if os.path.exists(PERSIST_DIRECTORY) and len(os.listdir(PERSIST_DIRECTORY)) > 0:
            print(f"üìÇ Ph√°t hi·ªán Vector Store c√≥ s·∫µn t·∫°i: {PERSIST_DIRECTORY}")
            print("‚è≥ ƒêang load database...\n")
            
            try:
                db = Chroma(
                    persist_directory=str(PERSIST_DIRECTORY),
                    embedding_function=embeddings
                )
                count = db._collection.count() if db._collection else 0
                
                if count > 0:
                    print(f"‚úÖ ƒê√£ load Vector Store th√†nh c√¥ng!")
                    print(f"   üìä S·ªë documents trong database: {count}\n")
                    should_create_new = False
                else:
                    print("   ‚ö†Ô∏è Database c√≥ s·∫µn nh∆∞ng R·ªñNG (0 docs). S·∫Ω t·∫°o l·∫°i...")
                    
            except Exception as e_db_load:
                print(f"\n‚ùå L·ªñI khi load Vector Store c√≥ s·∫µn: {e_db_load}")
                print(f"   Th·ª≠ x√≥a th∆∞ m·ª•c '{PERSIST_DIRECTORY}' v√† ch·∫°y l·∫°i ƒë·ªÉ t·∫°o m·ªõi.")
                return None, embeddings
        
        # ----- T·∫°o Database m·ªõi n·∫øu c·∫ßn -----
        if should_create_new:
            print(f"üÜï B·∫Øt ƒë·∫ßu t·∫°o m·ªõi Vector Store t·ª´ {CHUNKS_FILE.name}...\n")
            
            # Ki·ªÉm tra file t·ªìn t·∫°i
            if not CHUNKS_FILE.exists():
                print(f"‚ùå KH√îNG T√åM TH·∫§Y FILE DATA: {CHUNKS_FILE}")
                return None, embeddings
            
            try:
                # Load file chunks
                print("üìñ [1/4] ƒêang load file chunks...")
                loader = TextLoader(str(CHUNKS_FILE), encoding='utf-8')
                documents = loader.load()
                print(f"   ‚úì ƒê√£ load {len(documents)} document base")
                
                # Split documents
                print("‚úÇÔ∏è  [2/4] ƒêang split th√†nh t·ª´ng chunk v√† th√™m metadata...")
                text_splitter = RecursiveCharacterTextSplitter(
                    separators=["---"],
                    chunk_size=800,
                    chunk_overlap=100,
                    length_function=len
                )
                docs = text_splitter.split_documents(documents)
                
                if not docs:
                    print("   ‚ö†Ô∏è Kh√¥ng split ƒë∆∞·ª£c chunk n√†o. Ki·ªÉm tra file v√† separator.")
                    return None, embeddings
                
                # Th√™m metadata product_name cho m·ªói chunk
                for doc in docs:
                    product_name = extract_product_name(doc.page_content)
                    doc.metadata['product_name'] = product_name
                
                print(f"   ‚úì ƒê√£ split th√†nh {len(docs)} chunks v·ªõi metadata product_name")
                
                # T·∫°o embeddings v√† l∆∞u
                print("üíæ [3/4] ƒêang t·∫°o embeddings v√† l∆∞u v√†o database...")
                print("   (Qu√° tr√¨nh n√†y c√≥ th·ªÉ m·∫•t v√†i ph√∫t...)\n")
                
                start_time = time.time()
                batch_size = 50
                total_docs = len(docs)
                
                if total_docs == 0:
                    print("   ‚ö†Ô∏è Kh√¥ng c√≥ chunk n√†o ƒë·ªÉ th√™m v√†o database.")
                    return None, embeddings
                
                # Batch ƒë·∫ßu ti√™n - t·∫°o database
                print(f"   ‚è≥ ƒêang x·ª≠ l√Ω batch 1/{(total_docs-1)//batch_size + 1}...")
                db = Chroma.from_documents(
                    documents=docs[:batch_size],
                    embedding=embeddings,
                    persist_directory=str(PERSIST_DIRECTORY)
                )
                
                # C√°c batch ti·∫øp theo
                for i in range(batch_size, total_docs, batch_size):
                    batch_end = min(i + batch_size, total_docs)
                    batch_num = (i // batch_size) + 1
                    total_batches = (total_docs - 1) // batch_size + 1
                    
                    print(f"   ‚Üí Batch {batch_num}/{total_batches}: docs {i}-{batch_end}...", end='\r')
                    
                    try:
                        db.add_documents(docs[i:batch_end])
                    except Exception as batch_error:
                        print(f"\n   ‚ùå L·ªói batch {batch_num}: {batch_error}")
                    
                    # Gi·∫£i ph√≥ng b·ªô nh·ªõ GPU
                    if device == 'cuda':
                        torch.cuda.empty_cache()
                
                end_time = time.time()
                count_after = db._collection.count() if db._collection else 0
                
                print(f"\n   ‚úì Ho√†n th√†nh sau {end_time - start_time:.2f} gi√¢y")
                print(f"   üìä ƒê√£ t·∫°o v√† l∆∞u {count_after} vectors")
                
                if count_after != total_docs:
                    print(f"   ‚ö†Ô∏è C·∫£nh b√°o: S·ªë vector ({count_after}) kh√¥ng kh·ªõp s·ªë chunk ({total_docs})")
                
                print("\n‚úÖ ƒê√£ t·∫°o Vector Store th√†nh c√¥ng!")
                
            except FileNotFoundError as e_file:
                print(f"\n‚ùå L·ªñI: {e_file}")
                return None, embeddings
            except Exception as e_create:
                print(f"\n‚ùå L·ªñI khi t·∫°o Vector Store: {e_create}")
                import traceback
                traceback.print_exc()
                
                # X√≥a th∆∞ m·ª•c c√≥ th·ªÉ b·ªã t·∫°o d·ªü
                if os.path.exists(PERSIST_DIRECTORY):
                    try:
                        import shutil
                        shutil.rmtree(PERSIST_DIRECTORY)
                        print(f"   ƒê√£ x√≥a th∆∞ m·ª•c '{PERSIST_DIRECTORY}' c√≥ th·ªÉ b·ªã l·ªói.")
                    except:
                        pass
                return None, embeddings

        return db, embeddings
        
    except Exception as e_global:
        print(f"\n‚ùå ƒê√É X·∫¢Y RA L·ªñI KH√îNG X√ÅC ƒê·ªäNH: {e_global}")
        import traceback
        traceback.print_exc()
        return None, None

# =============================================================================
# RAG CHAIN SETUP - Enhanced Prompt & Format
# =============================================================================
def setup_rag_chain(db):
    """Setup RAG chain with comprehensive prompt and smart product grouping"""
    print("\n" + "=" * 80)
    print("‚õìÔ∏è KH·ªûI T·∫†O RAG CHAIN")
    print("=" * 80)
    
    if db is None:
        print("\n‚ùå L·ªñI: Vector store ch∆∞a ƒë∆∞·ª£c kh·ªüi t·∫°o th√†nh c√¥ng!")
        return None
    
    # 1. LLM Configuration (gi·ªØ nguy√™n th√¥ng s·ªë production)
    print("\nü§ñ [1/3] ƒêang k·∫øt n·ªëi v·ªõi Google Gemini...")
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.5-flash",
        temperature=0.05,
        max_output_tokens=1500,
        convert_system_message_to_human=True,
        request_options={"timeout": 90},
        max_retries=3
    )
    print("   ‚úì ƒê√£ k·∫øt n·ªëi Gemini 2.0 Flash (temperature=0.05, max_tokens=1500)")
    
    # 2. Retriever
    print("üîç [2/3] ƒêang t·∫°o Retriever...")
    retriever = db.as_retriever(
        search_type="similarity",
        search_kwargs={"k": 30}
    )
    print("   ‚úì Retriever: t√¨m 30 chunks relevant nh·∫•t (similarity search)")
    
    # 3. Enhanced Prompt Template (t·ª´ standalone)
    print("üìù [3/3] ƒêang t·∫°o Prompt Template...")
    
    template = """You are a strict assistant. You must answer questions based ONLY on the provided context below. DO NOT use your internal knowledge to update or guess prices. If the price is not mentioned in the context, say 'Price not available'.
B·∫°n l√† chuy√™n gia t∆∞ v·∫•n m·ªπ ph·∫©m chuy√™n nghi·ªáp, th√¢n thi·ªán v√† hi·ªÉu t√¢m l√Ω kh√°ch h√†ng.
PH√ÇN LO·∫†I C√ÇU H·ªéI V√Ä C√ÅCH TR·∫¢ L·ªúI:
üîπ **CH√ÄO H·ªéI/GIAO TI·∫æP C∆† B·∫¢N**
C√¢u h·ªèi: "xin ch√†o", "hi", "hello", "ch√†o b·∫°n", "hey"
‚Üí "Ch√†o b·∫°n! üëã M√¨nh l√† tr·ª£ l√Ω t∆∞ v·∫•n m·ªπ ph·∫©m. B·∫°n mu·ªën t√¨m s·∫£n ph·∫©m g√¨ h√¥m nay? üòä"
üîπ **H·ªéI V·ªÄ CH·ª®C NƒÇNG/GI·ªöI THI·ªÜU**
C√¢u h·ªèi: "b·∫°n l√† ai", "b·∫°n l√†m g√¨", "c√≥ th·ªÉ gi√∫p g√¨", "b·∫°n bi·∫øt g√¨"
‚Üí "M√¨nh l√† chuy√™n gia t∆∞ v·∫•n m·ªπ ph·∫©m! üíÑ M√¨nh c√≥ th·ªÉ gi√∫p b·∫°n:
‚Ä¢ T√¨m s·∫£n ph·∫©m theo lo·∫°i da (kh√¥, d·∫ßu, nh·∫°y c·∫£m, h·ªón h·ª£p, th∆∞·ªùng)
‚Ä¢ T∆∞ v·∫•n s·∫£n ph·∫©m theo B·ªÜNH DA (m·ª•n, ch√†m, v·∫£y n·∫øn, tr·ª©ng c√° ƒë·ªè, n·∫•m da...)
‚Ä¢ T∆∞ v·∫•n kem d∆∞·ª°ng, serum, toner, m·∫∑t n·∫°, s·ªØa r·ª≠a m·∫∑t, kem ch·ªëng n·∫Øng
‚Ä¢ Gi·∫£i th√≠ch th√†nh ph·∫ßn v√† c√¥ng d·ª•ng s·∫£n ph·∫©m
B·∫°n ƒëang g·∫∑p v·∫•n ƒë·ªÅ g√¨ v·ªÅ da ho·∫∑c c·∫ßn t√¨m s·∫£n ph·∫©m n√†o? üòä"
üîπ **H·ªéI CHUNG CHUNG KH√îNG C·ª§ TH·ªÇ**
C√¢u h·ªèi: "c√≥ s·∫£n ph·∫©m g√¨", "cho xem s·∫£n ph·∫©m", "g·ª£i √Ω s·∫£n ph·∫©m"
‚Üí "M√¨nh c√≥ r·∫•t nhi·ªÅu s·∫£n ph·∫©m! üòä ƒê·ªÉ t∆∞ v·∫•n ch√≠nh x√°c, b·∫°n cho m√¨nh bi·∫øt:
‚Ä¢ Lo·∫°i da c·ªßa b·∫°n? (kh√¥/d·∫ßu/h·ªón h·ª£p/nh·∫°y c·∫£m/th∆∞·ªùng)
‚Ä¢ B·ªánh da (n·∫øu c√≥)? (m·ª•n/ch√†m/v·∫£y n·∫øn/tr·ª©ng c√° ƒë·ªè/n·∫•m da...)
‚Ä¢ Lo·∫°i s·∫£n ph·∫©m c·∫ßn? (kem d∆∞·ª°ng/serum/toner/m·∫∑t n·∫°/s·ªØa r·ª≠a m·∫∑t...)
Cho m√¨nh bi·∫øt ƒë·ªÉ m√¨nh t∆∞ v·∫•n ƒë√∫ng nhu c·∫ßu nh√©! üíï"
üîπ **H·ªéI V·ªÄ B·ªÜNH DA (∆ØU TI√äN CAO)**
C√¢u h·ªèi: "t√¥i b·ªã m·ª•n", "t√¥i b·ªã ch√†m", "da b·ªã v·∫£y n·∫øn", "b·ªã tr·ª©ng c√° ƒë·ªè", "n·∫•m da", "m·ª•n c√≥c"...
‚ö†Ô∏è **KI·ªÇM TRA PH·∫†M VI T∆Ø V·∫§N:**
‚Üí CH·ªà t∆∞ v·∫•n cho C√ÅC B·ªÜNH DA SAU (c√≥ trong database):
   ‚Ä¢ M·ª•n (Acne) ‚Üí H·ªón h·ª£p/D·∫ßu/Nh·∫°y c·∫£m
   ‚Ä¢ Ch√†m (Eczema) ‚Üí H·ªón h·ª£p/Kh√¥/Th∆∞·ªùng/D·∫ßu/Nh·∫°y c·∫£m
   ‚Ä¢ V·∫£y n·∫øn (Psoriasis) ‚Üí Kh√¥
   ‚Ä¢ Tr·ª©ng c√° ƒë·ªè (Rosacea) ‚Üí H·ªón h·ª£p/D·∫ßu/Nh·∫°y c·∫£m
   ‚Ä¢ D√†y s·ª´ng (Actinic Keratosis) ‚Üí Kh√¥/Th∆∞·ªùng
   ‚Ä¢ N·∫•m da (Tinea) ‚Üí H·ªón h·ª£p/D·∫ßu
   ‚Ä¢ Vi√™m da ti·∫øt b√£ (Seborrheic Keratoses) ‚Üí Th∆∞·ªùng/D·∫ßu/Nh·∫°y c·∫£m
   ‚Ä¢ T·ªïn th∆∞∆°ng n·∫Øng (Sun Damage) ‚Üí H·ªón h·ª£p/Kh√¥/Th∆∞·ªùng/Nh·∫°y c·∫£m
   ‚Ä¢ M·ª•n c√≥c (Warts) ‚Üí H·ªón h·ª£p/Kh√¥/Th∆∞·ªùng/D·∫ßu/Nh·∫°y c·∫£m
   ‚Ä¢ Ph√°t ban thu·ªëc (Drug Eruption) ‚Üí H·ªón h·ª£p/Kh√¥/Th∆∞·ªùng/D·∫ßu/Nh·∫°y c·∫£m
‚Üí **N·∫æU B·ªÜNH DA KH√îNG TRONG DANH S√ÅCH TR√äN** (vd: gh·∫ª, lang ben, zona, herpes...):
   "‚ö†Ô∏è Xin l·ªói, b·ªánh [t√™n b·ªánh] N·∫∞M NGO√ÄI PH·∫†M VI t∆∞ v·∫•n m·ªπ ph·∫©m c·ªßa m√¨nh.
   
   üè• KHUY·∫æN C√ÅO:
   ‚Ä¢ ƒê√¢y l√† b·ªánh da C·∫¶N ƒêI·ªÄU TR·ªä Y KHOA
   ‚Ä¢ Vui l√≤ng ƒê·∫∂T L·ªäCH G·∫∂P B√ÅC Sƒ® DA LI·ªÑU ƒë·ªÉ ƒë∆∞·ª£c kh√°m v√† k√™ ƒë∆°n thu·ªëc ph√π h·ª£p
   
   üí° M√¨nh c√≥ th·ªÉ t∆∞ v·∫•n m·ªπ ph·∫©m cho c√°c v·∫•n ƒë·ªÅ da th√¥ng th∆∞·ªùng nh∆∞: m·ª•n, ch√†m, v·∫£y n·∫øn... B·∫°n c√≥ v·∫•n ƒë·ªÅ da n√†o trong s·ªë n√†y kh√¥ng?"
‚Üí **N·∫æU B·ªÜNH DA C√ì TRONG DANH S√ÅCH - TR·∫¢ L·ªúI NG·∫ÆN G·ªåN:**
   "D·∫°, m√¨nh g·ª£i √Ω s·∫£n ph·∫©m cho [t√™n b·ªánh] nh√©:
   [LI·ªÜT K√ä 2-3 S·∫¢N PH·∫®M NGAY]"
   
   ‚ö†Ô∏è QUAN TR·ªåNG: 
   ‚Ä¢ CH·ªà tr·∫£ l·ªùi v·ªÅ b·ªánh da ƒë∆∞·ª£c N√äU TRONG C√ÇU H·ªéI HI·ªÜN T·∫†I
   ‚Ä¢ KH√îNG ƒë∆∞·ª£c nh·∫Øc l·∫°i ho·∫∑c nh·∫ßm l·∫´n v·ªõi c√¢u h·ªèi tr∆∞·ªõc
   ‚Ä¢ KH√îNG c·∫ßn gi·∫£i th√≠ch d√†i d√≤ng
üîπ **H·ªéI V·ªÄ V·∫§N ƒê·ªÄ DA (KH√îNG PH·∫¢I B·ªÜNH)**
C√¢u h·ªèi: "da t√¥i kh√¥", "da d·∫ßu nhi·ªÅu", "da nh·∫°y c·∫£m"
‚Üí ƒêI TH·∫≤NG V√ÄO: "D·∫°, m√¨nh g·ª£i √Ω s·∫£n ph·∫©m cho da [lo·∫°i da] nh√©:
   [LI·ªÜT K√ä 2-3 S·∫¢N PH·∫®M NGAY]"
üîπ **H·ªéI THEO LO·∫†I S·∫¢N PH·∫®M**
C√¢u h·ªèi: "c√≥ kem d∆∞·ª°ng n√†o...", "serum g√¨ t·ªët", "toner cho da..."
‚Üí ƒêI TH·∫≤NG V√ÄO S·∫¢N PH·∫®M, m·∫∑c ƒë·ªãnh 2-3 s·∫£n ph·∫©m
üîπ **H·ªéI V·ªÄ TH∆Ø∆†NG HI·ªÜU**
C√¢u h·ªèi: "b·∫°n c√≥ [t√™n th∆∞∆°ng hi·ªáu] kh√¥ng"
‚Üí Ki·ªÉm tra database, n·∫øu c√≥ th√¨ li·ªát k√™, n·∫øu kh√¥ng: "M√¨nh ch∆∞a c√≥ th√¥ng tin v·ªÅ [brand] trong database. B·∫°n mu·ªën t∆∞ v·∫•n s·∫£n ph·∫©m theo lo·∫°i da kh√¥ng? üîç"
üîπ **H·ªéI GI√Å/MUA ·ªû ƒê√ÇU**
‚Üí "Xin l·ªói, m√¨nh ch·ªâ t∆∞ v·∫•n v·ªÅ s·∫£n ph·∫©m th√¥i nh√©. B·∫°n c√≥ th·ªÉ mua t·∫°i c√°c store ch√≠nh h√£ng. M√¨nh t∆∞ v·∫•n th√™m s·∫£n ph·∫©m kh√°c kh√¥ng? üòä"
üîπ **C·∫¢M ∆†N/T·∫†M BI·ªÜT**
‚Üí "Kh√¥ng c√≥ g√¨! üòä Ch√∫c b·∫°n c√≥ l√†n da ƒë·∫πp! H·∫πn g·∫∑p l·∫°i! üíï"
üîπ **C√ÇU H·ªéI NGO√ÄI L·ªÄ**
‚Üí "Xin l·ªói, m√¨nh ch·ªâ chuy√™n v·ªÅ m·ªπ ph·∫©m v√† skincare th√¥i üíÑ B·∫°n c√≥ mu·ªën h·ªèi v·ªÅ chƒÉm s√≥c da kh√¥ng?"
---
**CH√ö √ù KHI TR·∫¢ L·ªúI:**
- Lu√¥n TH√ÇN THI·ªÜN, d√πng "m√¨nh/b·∫°n"
- **NG·∫ÆN G·ªåN - ƒêI TH·∫≤NG V√ÄO S·∫¢N PH·∫®M**
- **‚ö†Ô∏è T·∫¨P TRUNG V√ÄO C√ÇU H·ªéI HI·ªÜN T·∫†I** - kh√¥ng nh·∫Øc c√¢u h·ªèi c≈©
- **GROUNDING:** CH·ªà G·ª¢I √ù s·∫£n ph·∫©m C√ì TRONG DATABASE
  ‚Ä¢ N·∫øu context ch·ª©a "KH√îNG T√åM TH·∫§Y S·∫¢N PH·∫®M" ‚Üí tr·∫£ l·ªùi:
    "Xin l·ªói, m√¨nh kh√¥ng t√¨m th·∫•y s·∫£n ph·∫©m ph√π h·ª£p. üòî B·∫°n th·ª≠ m√¥ t·∫£ chi ti·∫øt h∆°n?"
  ‚Ä¢ TUY·ªÜT ƒê·ªêI KH√îNG T·ª∞ B·ªäA s·∫£n ph·∫©m
- **S·ªê L∆Ø·ª¢NG:** M·∫∑c ƒë·ªãnh 2-3 s·∫£n ph·∫©m (t·ªëi ƒëa 3)
- **‚ö†Ô∏è FORMAT M·ªñI S·∫¢N PH·∫®M (B·∫ÆT BU·ªòC):**
  **S·ªë. T√™n s·∫£n ph·∫©m c·ªßa TH∆Ø∆†NG HI·ªÜU** 
  Gi√°: XXX.XXX VND | ƒê√°nh gi√°: X/5 | Lo·∫°i da: [...]
  
  ‚ö†Ô∏è B·∫ÆT BU·ªòC: T√™n, Th∆∞∆°ng hi·ªáu, Gi√°, ƒê√°nh gi√°, Lo·∫°i da
  ‚Ä¢ N·∫æU thi·∫øu th√¥ng tin ‚Üí ghi "(Kh√¥ng c√≥ th√¥ng tin)"
  ‚Ä¢ KH√îNG hi·ªÉn th·ªã c√¥ng d·ª•ng ho·∫∑c th√†nh ph·∫ßn
- **GI√Å:** CH·ªà HI·ªÇN TH·ªä VND, KH√îNG USD
- **LO·∫†I DA PH·∫¢I D·ªäCH SANG TI·∫æNG VI·ªÜT:**
  Combination ‚Üí H·ªón h·ª£p | Dry ‚Üí Kh√¥ | Normal ‚Üí Th∆∞·ªùng | Oily ‚Üí D·∫ßu | Sensitive ‚Üí Nh·∫°y c·∫£m
- D√πng emoji ph√π h·ª£p: üòäüíÑ‚ú®üíïüëã
TH√îNG TIN S·∫¢N PH·∫®M:
{context}
C√ÇU H·ªéI: {question}
TR·∫¢ L·ªúI (2-3 s·∫£n ph·∫©m):"""
    
    prompt = ChatPromptTemplate.from_template(template)
    print("   ‚úì ƒê√£ t·∫°o Prompt Template (comprehensive + grounding)")
    
    # 4. Format docs function v·ªõi metadata extraction
    def format_docs(docs):
        """
        Format documents: NH√ìM chunks theo product_name,
        tr√≠ch xu·∫•t metadata, s·∫Øp x·∫øp theo relevance,
        ch·ªçn top 3 s·∫£n ph·∫©m v·ªõi th√¥ng tin ƒë·∫ßy ƒë·ªß
        """
        # GROUNDING CHECK
        if not docs or len(docs) == 0:
            return "KH√îNG T√åM TH·∫§Y S·∫¢N PH·∫®M TRONG DATABASE"
        
        print(f"   üîç T√¨m ƒë∆∞·ª£c {len(docs)} chunks t·ª´ database")
        
        # B∆∞·ªõc 1: Nh√≥m chunks theo product_name
        product_groups = {}
        
        for idx, doc in enumerate(docs):
            product_name = doc.metadata.get('product_name', 'Unknown Product')
            
            if product_name not in product_groups:
                metadata = {
                    'brand': extract_field_from_chunk(doc.page_content, 'Brand'),
                    'category': extract_field_from_chunk(doc.page_content, 'Category'),
                    'suitable_for': extract_field_from_chunk(doc.page_content, 'Suitable for'),
                    'rank': extract_field_from_chunk(doc.page_content, 'Rank'),
                    'price': extract_field_from_chunk(doc.page_content, 'Price')
                }
                
                product_groups[product_name] = {
                    'chunks': [],
                    'first_index': idx,
                    'metadata': metadata,
                    'has_summary': False,
                    'has_ingredients': False
                }
            
            # ƒê√°nh d·∫•u lo·∫°i chunk
            content_lower = doc.page_content.lower()
            if 'chunk type: product summary' in content_lower:
                product_groups[product_name]['has_summary'] = True
            if 'chunk type: ingredients' in content_lower:
                product_groups[product_name]['has_ingredients'] = True
            
            product_groups[product_name]['chunks'].append(doc)
        
        if not product_groups:
            return "KH√îNG T√åM TH·∫§Y S·∫¢N PH·∫®M TRONG DATABASE"
        
        print(f"   üì¶ T√¨m ƒë∆∞·ª£c {len(product_groups)} s·∫£n ph·∫©m kh√°c nhau")
        
        # B∆∞·ªõc 2: L·ªçc s·∫£n ph·∫©m c√≥ ƒë·ªß th√¥ng tin (∆∞u ti√™n c√≥ summary)
        complete_products = [(name, data) for name, data in product_groups.items() if data['has_summary']]
        
        if not complete_products:
            complete_products = list(product_groups.items())
        
        # B∆∞·ªõc 3: S·∫Øp x·∫øp theo relevance (first_index)
        sorted_products = sorted(complete_products, key=lambda x: x[1]['first_index'])
        
        # B∆∞·ªõc 4: Ch·ªçn top 3 s·∫£n ph·∫©m
        num_products = min(3, len(sorted_products))
        selected_products = sorted_products[:num_products]
        
        print(f"   ‚úÖ Ch·ªçn {num_products} s·∫£n ph·∫©m ƒë·ªÉ t∆∞ v·∫•n")
        
        # B∆∞·ªõc 5: Format output
        formatted = []
        for i, (product_name, data) in enumerate(selected_products, 1):
            chunks = data['chunks']
            metadata = data['metadata']
            
            # Lo·∫°i b·ªè duplicate chunks
            seen_contents = set()
            unique_chunks = []
            for chunk in chunks:
                content_hash = hash(chunk.page_content.strip())
                if content_hash not in seen_contents:
                    seen_contents.add(content_hash)
                    unique_chunks.append(chunk)
            
            # S·∫Øp x·∫øp: Summary tr∆∞·ªõc, Ingredients sau
            def chunk_priority(chunk):
                content = chunk.page_content.lower()
                if 'chunk type: product summary' in content:
                    return 0
                elif 'chunk type: ingredients' in content:
                    return 1
                else:
                    return 2
            
            sorted_chunks = sorted(unique_chunks, key=chunk_priority)
            
            # G·ªôp th√¥ng tin s·∫£n ph·∫©m
            product_info = f"{'='*80}\n"
            product_info += f"S·∫¢N PH·∫®M #{i}: {product_name}\n"
            product_info += f"{'='*80}\n"
            
            # Metadata t·ªïng h·ª£p
            if metadata['brand']:
                product_info += f"üè¢ Th∆∞∆°ng hi·ªáu: {metadata['brand']}\n"
            if metadata['category']:
                product_info += f"üìÅ Lo·∫°i: {metadata['category']}\n"
            if metadata['suitable_for']:
                product_info += f"üë§ Ph√π h·ª£p: {metadata['suitable_for']}\n"
            if metadata['rank']:
                product_info += f"‚≠ê ƒê√°nh gi√°: {metadata['rank']}\n"
            if metadata['price']:
                price_vnd = convert_price_in_text(f"Price: {metadata['price']}")
                product_info += f"üí∞ {price_vnd}\n"
            
            product_info += f"{'-'*80}\n\n"
            
            # Th√™m n·ªôi dung chi ti·∫øt
            for chunk in sorted_chunks:
                content = chunk.page_content.strip()
                content = convert_price_in_text(content)
                product_info += content + "\n\n"
            
            formatted.append(product_info)
        
        return "\n\n".join(formatted)
    
    # Build RAG chain
    rag_chain = (
        {
            "context": retriever | format_docs,
            "question": RunnablePassthrough()
        }
        | prompt
        | llm
        | StrOutputParser()
    )
    
    print("\n‚úÖ RAG Chain ƒë√£ s·∫µn s√†ng!")
    print("\nüìä Lu·ªìng ho·∫°t ƒë·ªông (C·∫¢I TI·∫æN):")
    print("   1Ô∏è‚É£  User Question ‚Üí Retriever")
    print("   2Ô∏è‚É£  Retriever ‚Üí 30 chunks (similarity search)")
    print("   3Ô∏è‚É£  Tr√≠ch xu·∫•t metadata t·ª´ chunks")
    print("   4Ô∏è‚É£  NH√ìM theo product_name + Filter s·∫£n ph·∫©m c√≥ ƒë·ªß th√¥ng tin")
    print("   5Ô∏è‚É£  S·∫Øp x·∫øp theo relevance ‚Üí Ch·ªçn top 3 s·∫£n ph·∫©m")
    print("   6Ô∏è‚É£  Lo·∫°i b·ªè duplicate + S·∫Øp x·∫øp: Summary ‚Üí Ingredients")
    print("   7Ô∏è‚É£  Format structured v·ªõi metadata r√µ r√†ng")
    print("   8Ô∏è‚É£  Context + Question ‚Üí LLM ‚Üí 2-3 s·∫£n ph·∫©m CH√çNH X√ÅC ‚ö°")
    
    return rag_chain

# =============================================================================
# VISION ANALYSIS - STATELESS (Base64/Bytes/PIL/Path support)
# =============================================================================
def analyze_skin_image(image_input, note: str = None):
    """
    Analyze skin image - STATELESS version with severity detection
    Args:
        image_input: PIL Image, base64 string, bytes, or file path
        note: Additional note from user
    Returns:
        str: Analysis result
    """
    try:
        print("\nüì∏ Analyzing skin image...")
        
        # Handle multiple input types
        img = None
        if isinstance(image_input, str):
            # Check for data URI
            if image_input.startswith('data:image'):
                image_input = image_input.split(',')[1]
            try:
                # Try base64 decode
                image_bytes = base64.b64decode(image_input)
                img = Image.open(io.BytesIO(image_bytes))
            except:
                # Try file path
                img = Image.open(image_input)
        elif isinstance(image_input, Image.Image):
            img = image_input
        elif isinstance(image_input, bytes):
            img = Image.open(io.BytesIO(image_input))
        
        if img is None:
            raise ValueError("Invalid image input")

        vision_model = genai.GenerativeModel('gemini-2.5-flash')
        
        vision_prompt = """B·∫°n l√† chuy√™n gia da li·ªÖu. Ph√¢n t√≠ch ·∫£nh da v√† T√ìM T·∫ÆT NG·∫ÆN G·ªåN:
1. LO·∫†I DA: (kh√¥/d·∫ßu/h·ªón h·ª£p/nh·∫°y c·∫£m/th∆∞·ªùng)
2. V·∫§N ƒê·ªÄ CH√çNH & M·ª®C ƒê·ªò NGHI√äM TR·ªåNG:
- N·∫øu c√≥ m·ª•n: lo·∫°i m·ª•n (vi√™m/ƒë·∫ßu ƒëen/ƒë·∫ßu tr·∫Øng/b·ªçc), m·ª©c ƒë·ªô (NH·∫∏/TRUNG B√åNH/N·∫∂NG/R·∫§T N·∫∂NG)
- N·∫øu c√≥ th√¢m/s·∫πo: m·ª©c ƒë·ªô (NH·∫∏/TRUNG B√åNH/N·∫∂NG/R·∫§T N·∫∂NG), m√†u s·∫Øc, ph√¢n b·ªë
- N·∫øu c√≥ l√£o h√≥a: m·ª©c ƒë·ªô (NH·∫∏/TRUNG B√åNH/N·∫∂NG)
- N·∫øu c√≥ v·∫•n ƒë·ªÅ kh√°c: n√™u r√µ
3. M·ª®C ƒê·ªò CHUNG: Ch·ªçn 1 trong 4 (QUAN TR·ªåNG):
   - NH·∫∏: V·∫•n ƒë·ªÅ nh·ªè, √≠t n·ªët, c√≥ th·ªÉ t·ª± chƒÉm s√≥c
   - TRUNG B√åNH: V·∫•n ƒë·ªÅ r√µ r√†ng, nhi·ªÅu n·ªët, c·∫ßn s·∫£n ph·∫©m chuy√™n d·ª•ng
   - N·∫∂NG: V·∫•n ƒë·ªÅ lan r·ªông, vi√™m nhi·ªÅu, c·∫ßn ƒëi·ªÅu tr·ªã t√≠ch c·ª±c
   - R·∫§T N·∫∂NG: Vi√™m tr·∫ßm tr·ªçng, s·∫πo nhi·ªÅu, C·∫¶N G·∫∂P B√ÅC Sƒ®
4. G·ª¢I √ù: (1 c√¢u ng·∫Øn)
QUAN TR·ªåNG: Ph·∫£i ghi r√µ M·ª®C ƒê·ªò.
Tr·∫£ l·ªùi NG·∫ÆN G·ªåN, b·∫±ng ti·∫øng Vi·ªát."""

        if note:
            vision_prompt += f"\n\nGhi ch√∫ th√™m t·ª´ ng∆∞·ªùi d√πng: {note}"
        
        response = vision_model.generate_content([vision_prompt, img])
        analysis = response.text
        print("‚úÖ Analysis complete!")
        return analysis
        
    except FileNotFoundError:
        print(f"‚ùå Kh√¥ng t√¨m th·∫•y file ·∫£nh")
        return None
    except Exception as e:
        print(f"‚ùå Error analyzing image: {str(e)}")
        return None

# =============================================================================
# HELPER FUNCTIONS FOR NESTJS INTEGRATION
# =============================================================================
def analyze_with_context(question: str, conversation_history: list = None) -> str:
    """
    Analyze question with conversation context + Skin Condition Logic
    ‚ö†Ô∏è N·∫æU PH√ÅT HI·ªÜN B·ªÜNH DA ‚Üí B·ªé QUA CONTEXT ƒë·ªÉ tr√°nh nh·∫ßm l·∫´n
    """
    # Detect skin condition
    detected_condition, suitable_skin_types = detect_skin_condition_and_types(question)
    
    if detected_condition:
        # C√ì B·ªÜNH DA ‚Üí Kh√¥ng d√πng context, mapping sang ti·∫øng Anh
        skin_types_mapping = {
            "Kh√¥": "Dry",
            "Th∆∞·ªùng": "Normal",
            "D·∫ßu": "Oily",
            "Nh·∫°y c·∫£m": "Sensitive",
            "H·ªón h·ª£p": "Combination"
        }
        
        english_skin_types = [skin_types_mapping.get(st, st) for st in suitable_skin_types]
        skin_query = " ".join(english_skin_types)
        enhanced_query = f"{detected_condition} {skin_query} skin treatment moisturizer serum toner cream"
        
        return enhanced_query
    
    # KH√îNG C√ì B·ªÜNH DA ‚Üí D√πng context b√¨nh th∆∞·ªùng
    context_str = ""
    if conversation_history:
        recent_context = conversation_history[-3:]
        context_str = "L·ªäCH S·ª¨ H·ªòI THO·∫†I G·∫¶N ƒê√ÇY:\n" + "\n".join([
            f"User: {ctx[0]}\nBot: {ctx[1][:200]}..."
            for ctx in recent_context
        ])

    return f"""{context_str}
C√ÇU H·ªéI HI·ªÜN T·∫†I: {question}
H√£y tr·∫£ l·ªùi d·ª±a tr√™n c√¢u h·ªèi hi·ªán t·∫°i. Ch·ªâ tham kh·∫£o l·ªãch s·ª≠ n·∫øu user ƒëang h·ªèi ti·∫øp v·ªÅ c√πng topic."""

def build_image_analysis_query(skin_analysis: str, additional_text: str = None) -> str:
    """Build RAG query based on Image Analysis Result with severity awareness"""
    is_severe = any(keyword in skin_analysis.upper() for keyword in ['R·∫§T N·∫∂NG', 'R·∫§T NGHI√äM TR·ªåNG', 'C·∫¶N G·∫∂P B√ÅC Sƒ®'])
    
    warning = "(R·∫§T NGHI√äM TR·ªåNG - C·∫¶N G·∫∂P B√ÅC Sƒ®)" if is_severe else "(t·ª´ ph√¢n t√≠ch ·∫£nh)"
    advice_req = "G·ª£i √Ω 1-2 s·∫£n ph·∫©m H·ªñ TR·ª¢ NH·∫∏ NH√ÄNG. NH·∫§N M·∫†NH: C·∫ßn g·∫∑p b√°c sƒ©." if is_severe else "T∆∞ v·∫•n 2-3 s·∫£n ph·∫©m C·ª§ TH·ªÇ ph√π h·ª£p v·ªõi M·ª®C ƒê·ªò."
    
    user_req = f"\nY√™u c·∫ßu th√™m c·ªßa user: {additional_text}" if additional_text else ""
    
    return f"""T√¨nh tr·∫°ng da {warning}:
{skin_analysis}
{user_req}
{advice_req}"""

def check_severity(analysis: str) -> bool:
    """Check if skin condition is severe"""
    if not analysis:
        return False
    return any(keyword in analysis.upper() for keyword in ['R·∫§T N·∫∂NG', 'R·∫§T NGHI√äM TR·ªåNG'])

# =============================================================================
# PRODUCT SUGGESTION HELPERS
# =============================================================================
def get_product_suggestions_by_skin_types(db, skin_types: list, num_products: int = 5) -> list:
    """
    Truy v·∫•n s·∫£n ph·∫©m ph√π h·ª£p v·ªõi lo·∫°i da (bilingual search)
    Returns: list of product names
    """
    if not db or not skin_types:
        print("‚ö†Ô∏è No database or skin types provided")
        return []
    
    try:
        print(f"üîç Searching products for skin types: {skin_types}")
        
        # Map ti·∫øng Vi·ªát sang ti·∫øng Anh
        vietnamese_to_english = {
            "Kh√¥": "Dry",
            "Th∆∞·ªùng": "Normal",
            "D·∫ßu": "Oily",
            "H·ªón h·ª£p": "Combination",
            "Nh·∫°y c·∫£m": "Sensitive"
        }
        
        # T·∫°o search terms (c·∫£ VN v√† EN)
        search_terms = []
        for skin_type in skin_types:
            search_terms.append(skin_type)
            if skin_type in vietnamese_to_english:
                search_terms.append(vietnamese_to_english[skin_type])
        
        print(f"üîç Search terms (VN + EN): {search_terms}")
        
        query = f"s·∫£n ph·∫©m chƒÉm s√≥c da {' '.join(search_terms)}"
        
        retriever = db.as_retriever(
            search_type="mmr",
            search_kwargs={
                "k": num_products * 5,
                "fetch_k": num_products * 10,
                "lambda_mult": 0.5
            }
        )
        
        docs = retriever.invoke(query)
        print(f"üìö Retrieved {len(docs)} documents from vector store")
        
        product_names = []
        seen_products = set()
        
        for doc in docs:
            product_name = doc.metadata.get('product_name')
            
            if not product_name:
                content_lines = doc.page_content.split('\n')
                for line in content_lines:
                    if 'Product Name:' in line:
                        product_name = line.split(':', 1)[1].strip()
                        break
            
            if product_name and product_name not in seen_products:
                content_lower = doc.page_content.lower()
                metadata_str = str(doc.metadata).lower()
                
                match = any(
                    term.lower() in content_lower or
                    term.lower() in metadata_str
                    for term in search_terms
                )
                
                if match:
                    product_names.append(product_name)
                    seen_products.add(product_name)
                    print(f"‚úì Found: {product_name}")
                    
                    if len(product_names) >= num_products:
                        break
        
        # Fallback: add general products if not enough
        if len(product_names) < num_products:
            print(f"‚ö†Ô∏è Only found {len(product_names)} matching products, adding general...")
            for doc in docs:
                product_name = doc.metadata.get('product_name')
                if not product_name:
                    content_lines = doc.page_content.split('\n')
                    for line in content_lines:
                        if 'Product Name:' in line:
                            product_name = line.split(':', 1)[1].strip()
                            break
                
                if product_name and product_name not in seen_products:
                    product_names.append(product_name)
                    seen_products.add(product_name)
                    print(f"‚úì Added general: {product_name}")
                    if len(product_names) >= num_products:
                        break
        
        print(f"‚úÖ Returning {len(product_names)} product suggestions")
        return product_names[:num_products]
        
    except Exception as e:
        print(f"‚ùå Error getting product suggestions: {e}")
        import traceback
        traceback.print_exc()
        return []

def map_disease_to_skin_types(disease_class: str) -> list:
    """Map disease class sang skin types ph√π h·ª£p"""
    print(f"üîç Mapping disease: {disease_class}")
    
    disease_lower = disease_class.lower().replace('_', ' ')
    
    disease_mapping = {
        'acne': ['H·ªón h·ª£p', 'D·∫ßu', 'Nh·∫°y c·∫£m'],
        'actinic keratosis': ['Kh√¥', 'Th∆∞·ªùng'],
        'drug eruption': ['H·ªón h·ª£p', 'Kh√¥', 'Th∆∞·ªùng', 'D·∫ßu', 'Nh·∫°y c·∫£m'],
        'eczema': ['H·ªón h·ª£p', 'Kh√¥', 'Th∆∞·ªùng', 'D·∫ßu', 'Nh·∫°y c·∫£m'],
        'psoriasis': ['Kh√¥'],
        'rosacea': ['H·ªón h·ª£p', 'D·∫ßu', 'Nh·∫°y c·∫£m'],
        'seborrh keratoses': ['Th∆∞·ªùng', 'D·∫ßu', 'Nh·∫°y c·∫£m'],
        'sun sunlight damage': ['H·ªón h·ª£p', 'Kh√¥', 'Th∆∞·ªùng', 'Nh·∫°y c·∫£m'],
        'tinea': ['H·ªón h·ª£p', 'D·∫ßu'],
        'warts': ['H·ªón h·ª£p', 'Kh√¥', 'Th∆∞·ªùng', 'D·∫ßu', 'Nh·∫°y c·∫£m'],
        'normal': ['Th∆∞·ªùng']
    }
    
    for key, skin_types in disease_mapping.items():
        if key in disease_lower or disease_lower in key:
            print(f"‚úì Mapped to skin types: {skin_types}")
            return skin_types
    
    # Fallback to SKIN_CONDITION_TO_SKIN_TYPE
    for condition_key, skin_types in SKIN_CONDITION_TO_SKIN_TYPE.items():
        if condition_key in disease_lower or disease_lower in condition_key:
            print(f"‚úì Mapped via SKIN_CONDITION_TO_SKIN_TYPE: {skin_types}")
            return skin_types
    
    print(f"‚ö†Ô∏è No specific mapping found, using default")
    return ["H·ªón h·ª£p", "Kh√¥", "Th∆∞·ªùng", "D·∫ßu", "Nh·∫°y c·∫£m"]